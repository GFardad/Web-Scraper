version: '3.8'

# ═══════════════════════════════════════════════════════════════════
# ENTERPRISE SCRAPER - DOCKER COMPOSE
# GPU-Accelerated, Local-First Architecture
# ═══════════════════════════════════════════════════════════════════
#
# Hardware Profile: i9-11900K | RTX 3060 Ti (8GB VRAM) | 32GB RAM
# VRAM Budget: 3GB total (Phi-3 Mini + PaddleOCR)
#
# One-Command Deployment:
#   docker-compose up -d
#
# Services: 7 containers
#   - scraper (main app with Playwright)
#   - postgres (structured data)
#   - mongo (raw HTML, screenshots, logs)
#   - redis (task queue)
#   - ollama (local LLM with GPU)
#   - paddleocr (OCR with GPU)
#   - prometheus (metrics)
#
# ═══════════════════════════════════════════════════════════════════

services:
  # ─────────────────────────────────────────────────────────────────
  # SCRAPER - Main Application
  # ─────────────────────────────────────────────────────────────────
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: scraper-app
    restart: unless-stopped
    
    # Mount config.yaml for hot-reload
    volumes:
      - ./config.yaml:/app/config.yaml:ro  # Read-only config
      - ./prompts:/app/prompts:ro  # LLM prompts
      - scraper-data:/app/data  # Persistent data
      - scraper-logs:/app/logs  # Logs
      - sessions:/app/data/browser_sessions  # Browser sessions
      - screenshots:/app/data/screenshots  # Screenshots
      - errors:/app/data/error_screenshots  # Error screenshots
      
    environment:
      # Database passwords (from .env file)
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      MONGO_PASSWORD: ${MONGO_PASSWORD}
      REDIS_PASSWORD: ${REDIS_PASSWORD}
      
      # AI service settings
      GPU_ENABLED: "true"
      MAX_VRAM_GB: "3.0"
      
      # Optional: Paid service API keys (last resort)
      BRIGHTDATA_API_KEY: ${BRIGHTDATA_API_KEY:-}
      CAPTCHA_API_KEY: ${CAPTCHA_API_KEY:-}
      TELEGRAM_BOT_TOKEN: ${TELEGRAM_BOT_TOKEN:-}
      TELEGRAM_CHAT_ID: ${TELEGRAM_CHAT_ID:-}
      
    depends_on:
      postgres:
        condition: service_healthy
      mongo:
        condition: service_healthy
      redis:
        condition: service_healthy
      ollama:
        condition: service_started
        
    networks:
      - scraper-network
      
    ports:
      - "8080:8080"  # Health check endpoint
      - "9090:9090"  # Prometheus metrics
      
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ─────────────────────────────────────────────────────────────────
  # POSTGRESQL - Structured Data
  # ─────────────────────────────────────────────────────────────────
  postgres:
    image: postgres:16-alpine
    container_name: scraper-postgres
    restart: unless-stopped
    
    environment:
      POSTGRES_DB: scraper_db
      POSTGRES_USER: scraper
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF-8"
      
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/init_postgres.sql:/docker-entrypoint-initdb.d/init.sql:ro
      
    networks:
      - scraper-network
      
    ports:
      - "127.0.0.1:5432:5432"  # Localhost only
      
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U scraper"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ─────────────────────────────────────────────────────────────────
  # MONGODB - Raw Data & Flexible Schemas
  # ─────────────────────────────────────────────────────────────────
  mongo:
    image: mongo:7.0
    container_name: scraper-mongo
    restart: unless-stopped
    
    environment:
      MONGO_INITDB_ROOT_USERNAME: scraper
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}
      MONGO_INITDB_DATABASE: scraper_raw
      
    volumes:
      - mongo-data:/data/db
      - ./docker/mongo/init_mongo.js:/docker-entrypoint-initdb.d/init.js:ro
      
    networks:
      - scraper-network
      
    ports:
      - "127.0.0.1:27017:27017"  # Localhost only
      
    healthcheck:
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      
    # Increase WiredTiger cache for better performance
    command: --wiredTigerCacheSizeGB 4

  # ─────────────────────────────────────────────────────────────────
  # REDIS - Task Queue & Caching
  # ─────────────────────────────────────────────────────────────────
  redis:
    image: redis:7-alpine
    container_name: scraper-redis
    restart: unless-stopped
    
    command: redis-server --requirepass ${REDIS_PASSWORD} --maxmemory 2gb --maxmemory-policy allkeys-lru
    
    volumes:
      - redis-data:/data
      
    networks:
      - scraper-network
      
    ports:
      - "127.0.0.1:6379:6379"  # Localhost only
      
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ─────────────────────────────────────────────────────────────────
  # OLLAMA - Local LLM (GPU-Accelerated)
  # ─────────────────────────────────────────────────────────────────
  ollama:
    image: ollama/ollama:latest
    container_name: scraper-ollama
    restart: unless-stopped
    
    # GPU passthrough (requires NVIDIA Container Toolkit)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              
    environment:
      OLLAMA_NUM_PARALLEL: 1  # Single request at a time (VRAM limit)
      OLLAMA_MAX_LOADED_MODELS: 1  # Keep only one model in memory
      OLLAMA_FLASH_ATTENTION: 1  # Enable flash attention for efficiency
      
    volumes:
      - ollama-models:/root/.ollama  # Model storage
      
    networks:
      - scraper-network
      
    ports:
      - "127.0.0.1:11434:11434"  # API endpoint
      
    # Pull Phi-3 Mini model on startup
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 5
        ollama pull phi3:mini
        wait

  # ─────────────────────────────────────────────────────────────────
  # PADDLEOCR - OCR Engine (GPU-Accelerated)
  # ─────────────────────────────────────────────────────────────────
  paddleocr:
    image: paddlepaddle/paddle:2.5.2-gpu-cuda11.7-cudnn8.4-trt8.4
    container_name: scraper-paddleocr
    restart: unless-stopped
    
    # GPU passthrough
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
              
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      
    volumes:
      - ./docker/paddleocr:/app:ro
      - paddleocr-models:/root/.paddleocr  # Model cache
      
    networks:
      - scraper-network
      
    ports:
      - "127.0.0.1:8000:8000"  # OCR API
      
    working_dir: /app
    command: python ocr_server.py

  # ─────────────────────────────────────────────────────────────────
  # PROMETHEUS - Metrics Collection
  # ─────────────────────────────────────────────────────────────────
  prometheus:
    image: prom/prometheus:latest
    container_name: scraper-prometheus
    restart: unless-stopped
    
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
      
    networks:
      - scraper-network
      
    ports:
      - "9091:9090"  # Prometheus UI
      
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=90d'

# ═══════════════════════════════════════════════════════════════════
# NETWORKS
# ═══════════════════════════════════════════════════════════════════
networks:
  scraper-network:
    driver: bridge
    name: scraper-network

# ═══════════════════════════════════════════════════════════════════
# VOLUMES - Total Persistence
# ═══════════════════════════════════════════════════════════════════
volumes:
  # Scraper volumes
  scraper-data:
    name: scraper-data
  scraper-logs:
    name: scraper-logs
  sessions:
    name: scraper-sessions
  screenshots:
    name: scraper-screenshots
  errors:
    name: scraper-errors
    
  # Database volumes
  postgres-data:
    name: postgres-data
  mongo-data:
    name: mongo-data
  redis-data:
    name: redis-data
    
  # AI model volumes
  ollama-models:
    name: ollama-models
  paddleocr-models:
    name: paddleocr-models
    
  # Monitoring volumes
  prometheus-data:
    name: prometheus-data
